{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read patent high level mapping and claims\n",
    "claims = pd.read_csv(\"../data/claims_test.csv\", encoding = 'utf8')\n",
    "#patent_spec_map = pd.read_csv(\"../data/patent_spec_map.csv\", encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"claim size: \", len(set(claims[\"PATENT_ID\"])))\n",
    "print(\"patent size: \", len(set(patent_spec_map[\"PATENT_ID\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import tokenize\n",
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from pprint import pprint\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> import logging\n",
    ">>> logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process data into a document corpus and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_process(sent):\n",
    "    words = str(sent).split()\n",
    "\n",
    "    # 1. alphanumeric only\n",
    "    alphanum_only = [re.sub(\"[^a-zA-Z0-9]\", \"\", w) for w in words]\n",
    "\n",
    "    # 2. remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    meaningful_words = [w for w in alphanum_only if not w in stops]\n",
    "\n",
    "    # 3. lemmatization\n",
    "    words_lemma = [lemma.lemmatize(w) for w in meaningful_words]\n",
    "\n",
    "    return words_lemma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patent_corpus = {}\n",
    "for i, v in claims.iterrows():\n",
    "    pid = v[\"PATENT_ID\"]\n",
    "    text = sentence_process(v[\"CLAIM_TEXT\"])\n",
    "    if(pid not in patent_corpus):\n",
    "        patent_corpus[pid] = text\n",
    "    else:\n",
    "        patent_corpus[pid].extend(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del claims\n",
    "#del patent_spec_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Document Embedding class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TaggedPatentDocument:\n",
    "    def __init__(self, patent):\n",
    "        self.patent = patent\n",
    "    def __iter__(self):\n",
    "        # try only 10 documents\n",
    "        for pid, content in self.patent.items():\n",
    "            yield TaggedDocument(content, [pid])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patent_docs = TaggedPatentDocument(patent_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling min_cout size for vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-28 14:19:05,198 : INFO : collecting all words and their counts\n",
      "2017-08-28 14:19:05,200 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2017-08-28 14:19:05,203 : INFO : collected 444 word types and 86897599 unique tags from a corpus of 9 examples and 2838 words\n"
     ]
    }
   ],
   "source": [
    "pre = Doc2Vec(min_count=0)\n",
    "pre.scan_vocab(patent_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-28 14:19:08,936 : INFO : Loading a fresh vocabulary\n",
      "2017-08-28 14:19:08,939 : INFO : min_count=0 retains 444 unique words (100% of original 444, drops 0)\n",
      "2017-08-28 14:19:08,941 : INFO : min_count=0 leaves 2838 word corpus (100% of original 2838, drops 0)\n",
      "2017-08-28 14:19:08,952 : INFO : sample=0.001 downsamples 93 most-common words\n",
      "2017-08-28 14:19:08,954 : INFO : downsampling leaves estimated 1790 word corpus (63.1% of prior 2838)\n",
      "2017-08-28 14:19:08,956 : INFO : estimated required memory for 444 words and 100 dimensions: 34759616800 bytes\n",
      "2017-08-28 14:19:08,958 : INFO : Loading a fresh vocabulary\n",
      "2017-08-28 14:19:08,961 : INFO : min_count=1 retains 444 unique words (100% of original 444, drops 0)\n",
      "2017-08-28 14:19:08,963 : INFO : min_count=1 leaves 2838 word corpus (100% of original 2838, drops 0)\n",
      "2017-08-28 14:19:08,967 : INFO : sample=0.001 downsamples 93 most-common words\n",
      "2017-08-28 14:19:08,969 : INFO : downsampling leaves estimated 1790 word corpus (63.1% of prior 2838)\n",
      "2017-08-28 14:19:08,972 : INFO : estimated required memory for 444 words and 100 dimensions: 34759616800 bytes\n",
      "2017-08-28 14:19:08,975 : INFO : Loading a fresh vocabulary\n",
      "2017-08-28 14:19:08,978 : INFO : min_count=2 retains 285 unique words (64% of original 444, drops 159)\n",
      "2017-08-28 14:19:08,981 : INFO : min_count=2 leaves 2679 word corpus (94% of original 2838, drops 159)\n",
      "2017-08-28 14:19:08,984 : INFO : sample=0.001 downsamples 93 most-common words\n",
      "2017-08-28 14:19:08,986 : INFO : downsampling leaves estimated 1596 word corpus (59.6% of prior 2679)\n",
      "2017-08-28 14:19:08,989 : INFO : estimated required memory for 285 words and 100 dimensions: 34759410100 bytes\n",
      "2017-08-28 14:19:08,993 : INFO : Loading a fresh vocabulary\n",
      "2017-08-28 14:19:08,997 : INFO : min_count=3 retains 207 unique words (46% of original 444, drops 237)\n",
      "2017-08-28 14:19:08,999 : INFO : min_count=3 leaves 2523 word corpus (88% of original 2838, drops 315)\n",
      "2017-08-28 14:19:09,002 : INFO : sample=0.001 downsamples 109 most-common words\n",
      "2017-08-28 14:19:09,003 : INFO : downsampling leaves estimated 1402 word corpus (55.6% of prior 2523)\n",
      "2017-08-28 14:19:09,005 : INFO : estimated required memory for 207 words and 100 dimensions: 34759308700 bytes\n",
      "2017-08-28 14:19:09,009 : INFO : Loading a fresh vocabulary\n",
      "2017-08-28 14:19:09,011 : INFO : min_count=4 retains 172 unique words (38% of original 444, drops 272)\n",
      "2017-08-28 14:19:09,013 : INFO : min_count=4 leaves 2418 word corpus (85% of original 2838, drops 420)\n",
      "2017-08-28 14:19:09,015 : INFO : sample=0.001 downsamples 109 most-common words\n",
      "2017-08-28 14:19:09,017 : INFO : downsampling leaves estimated 1270 word corpus (52.5% of prior 2418)\n",
      "2017-08-28 14:19:09,019 : INFO : estimated required memory for 172 words and 100 dimensions: 34759263200 bytes\n",
      "2017-08-28 14:19:09,022 : INFO : Loading a fresh vocabulary\n",
      "2017-08-28 14:19:09,025 : INFO : min_count=5 retains 139 unique words (31% of original 444, drops 305)\n",
      "2017-08-28 14:19:09,027 : INFO : min_count=5 leaves 2286 word corpus (80% of original 2838, drops 552)\n",
      "2017-08-28 14:19:09,029 : INFO : sample=0.001 downsamples 125 most-common words\n",
      "2017-08-28 14:19:09,031 : INFO : downsampling leaves estimated 1104 word corpus (48.3% of prior 2286)\n",
      "2017-08-28 14:19:09,034 : INFO : estimated required memory for 139 words and 100 dimensions: 34759220300 bytes\n",
      "2017-08-28 14:19:09,036 : INFO : Loading a fresh vocabulary\n",
      "2017-08-28 14:19:09,038 : INFO : min_count=6 retains 125 unique words (28% of original 444, drops 319)\n",
      "2017-08-28 14:19:09,042 : INFO : min_count=6 leaves 2216 word corpus (78% of original 2838, drops 622)\n",
      "2017-08-28 14:19:09,044 : INFO : sample=0.001 downsamples 125 most-common words\n",
      "2017-08-28 14:19:09,046 : INFO : downsampling leaves estimated 1013 word corpus (45.8% of prior 2216)\n",
      "2017-08-28 14:19:09,048 : INFO : estimated required memory for 125 words and 100 dimensions: 34759202100 bytes\n",
      "2017-08-28 14:19:09,051 : INFO : Loading a fresh vocabulary\n",
      "2017-08-28 14:19:09,054 : INFO : min_count=7 retains 109 unique words (24% of original 444, drops 335)\n",
      "2017-08-28 14:19:09,056 : INFO : min_count=7 leaves 2120 word corpus (74% of original 2838, drops 718)\n",
      "2017-08-28 14:19:09,060 : INFO : sample=0.001 downsamples 109 most-common words\n",
      "2017-08-28 14:19:09,062 : INFO : downsampling leaves estimated 894 word corpus (42.2% of prior 2120)\n",
      "2017-08-28 14:19:09,064 : INFO : estimated required memory for 109 words and 100 dimensions: 34759181300 bytes\n",
      "2017-08-28 14:19:09,066 : INFO : Loading a fresh vocabulary\n",
      "2017-08-28 14:19:09,069 : INFO : min_count=8 retains 93 unique words (20% of original 444, drops 351)\n",
      "2017-08-28 14:19:09,072 : INFO : min_count=8 leaves 2008 word corpus (70% of original 2838, drops 830)\n",
      "2017-08-28 14:19:09,077 : INFO : sample=0.001 downsamples 93 most-common words\n",
      "2017-08-28 14:19:09,079 : INFO : downsampling leaves estimated 772 word corpus (38.5% of prior 2008)\n",
      "2017-08-28 14:19:09,081 : INFO : estimated required memory for 93 words and 100 dimensions: 34759160500 bytes\n",
      "2017-08-28 14:19:09,084 : INFO : Loading a fresh vocabulary\n",
      "2017-08-28 14:19:09,086 : INFO : min_count=9 retains 86 unique words (19% of original 444, drops 358)\n",
      "2017-08-28 14:19:09,088 : INFO : min_count=9 leaves 1952 word corpus (68% of original 2838, drops 886)\n",
      "2017-08-28 14:19:09,090 : INFO : sample=0.001 downsamples 86 most-common words\n",
      "2017-08-28 14:19:09,093 : INFO : downsampling leaves estimated 717 word corpus (36.8% of prior 1952)\n",
      "2017-08-28 14:19:09,094 : INFO : estimated required memory for 86 words and 100 dimensions: 34759151400 bytes\n",
      "2017-08-28 14:19:09,097 : INFO : Loading a fresh vocabulary\n",
      "2017-08-28 14:19:09,101 : INFO : min_count=10 retains 81 unique words (18% of original 444, drops 363)\n",
      "2017-08-28 14:19:09,102 : INFO : min_count=10 leaves 1907 word corpus (67% of original 2838, drops 931)\n",
      "2017-08-28 14:19:09,105 : INFO : sample=0.001 downsamples 81 most-common words\n",
      "2017-08-28 14:19:09,110 : INFO : downsampling leaves estimated 677 word corpus (35.5% of prior 1907)\n",
      "2017-08-28 14:19:09,113 : INFO : estimated required memory for 81 words and 100 dimensions: 34759144900 bytes\n",
      "2017-08-28 14:19:09,116 : INFO : Loading a fresh vocabulary\n",
      "2017-08-28 14:19:09,119 : INFO : min_count=11 retains 74 unique words (16% of original 444, drops 370)\n",
      "2017-08-28 14:19:09,121 : INFO : min_count=11 leaves 1837 word corpus (64% of original 2838, drops 1001)\n",
      "2017-08-28 14:19:09,124 : INFO : sample=0.001 downsamples 74 most-common words\n",
      "2017-08-28 14:19:09,126 : INFO : downsampling leaves estimated 619 word corpus (33.7% of prior 1837)\n",
      "2017-08-28 14:19:09,130 : INFO : estimated required memory for 74 words and 100 dimensions: 34759135800 bytes\n",
      "2017-08-28 14:19:09,132 : INFO : Loading a fresh vocabulary\n",
      "2017-08-28 14:19:09,135 : INFO : min_count=12 retains 68 unique words (15% of original 444, drops 376)\n",
      "2017-08-28 14:19:09,137 : INFO : min_count=12 leaves 1771 word corpus (62% of original 2838, drops 1067)\n",
      "2017-08-28 14:19:09,141 : INFO : sample=0.001 downsamples 68 most-common words\n",
      "2017-08-28 14:19:09,144 : INFO : downsampling leaves estimated 568 word corpus (32.1% of prior 1771)\n",
      "2017-08-28 14:19:09,147 : INFO : estimated required memory for 68 words and 100 dimensions: 34759128000 bytes\n",
      "2017-08-28 14:19:09,149 : INFO : Loading a fresh vocabulary\n",
      "2017-08-28 14:19:09,152 : INFO : min_count=13 retains 64 unique words (14% of original 444, drops 380)\n",
      "2017-08-28 14:19:09,154 : INFO : min_count=13 leaves 1723 word corpus (60% of original 2838, drops 1115)\n",
      "2017-08-28 14:19:09,156 : INFO : sample=0.001 downsamples 64 most-common words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_count: 0, size of vocab:  317.14285714285717\n",
      "min_count: 1, size of vocab:  317.14285714285717\n",
      "min_count: 2, size of vocab:  203.57142857142858\n",
      "min_count: 3, size of vocab:  147.85714285714286\n",
      "min_count: 4, size of vocab:  122.85714285714286\n",
      "min_count: 5, size of vocab:  99.28571428571429\n",
      "min_count: 6, size of vocab:  89.28571428571429\n",
      "min_count: 7, size of vocab:  77.85714285714286\n",
      "min_count: 8, size of vocab:  66.42857142857143\n",
      "min_count: 9, size of vocab:  61.42857142857143\n",
      "min_count: 10, size of vocab:  57.857142857142854\n",
      "min_count: 11, size of vocab:  52.857142857142854\n",
      "min_count: 12, size of vocab:  48.57142857142857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-28 14:19:09,159 : INFO : downsampling leaves estimated 534 word corpus (31.0% of prior 1723)\n",
      "2017-08-28 14:19:09,164 : INFO : estimated required memory for 64 words and 100 dimensions: 34759122800 bytes\n",
      "2017-08-28 14:19:09,166 : INFO : Loading a fresh vocabulary\n",
      "2017-08-28 14:19:09,169 : INFO : min_count=14 retains 62 unique words (13% of original 444, drops 382)\n",
      "2017-08-28 14:19:09,172 : INFO : min_count=14 leaves 1697 word corpus (59% of original 2838, drops 1141)\n",
      "2017-08-28 14:19:09,177 : INFO : sample=0.001 downsamples 62 most-common words\n",
      "2017-08-28 14:19:09,180 : INFO : downsampling leaves estimated 516 word corpus (30.4% of prior 1697)\n",
      "2017-08-28 14:19:09,182 : INFO : estimated required memory for 62 words and 100 dimensions: 34759120200 bytes\n",
      "2017-08-28 14:19:09,184 : INFO : Loading a fresh vocabulary\n",
      "2017-08-28 14:19:09,195 : INFO : min_count=15 retains 56 unique words (12% of original 444, drops 388)\n",
      "2017-08-28 14:19:09,199 : INFO : min_count=15 leaves 1613 word corpus (56% of original 2838, drops 1225)\n",
      "2017-08-28 14:19:09,202 : INFO : sample=0.001 downsamples 56 most-common words\n",
      "2017-08-28 14:19:09,209 : INFO : downsampling leaves estimated 462 word corpus (28.7% of prior 1613)\n",
      "2017-08-28 14:19:09,210 : INFO : estimated required memory for 56 words and 100 dimensions: 34759112400 bytes\n",
      "2017-08-28 14:19:09,213 : INFO : Loading a fresh vocabulary\n",
      "2017-08-28 14:19:09,216 : INFO : min_count=16 retains 51 unique words (11% of original 444, drops 393)\n",
      "2017-08-28 14:19:09,217 : INFO : min_count=16 leaves 1538 word corpus (54% of original 2838, drops 1300)\n",
      "2017-08-28 14:19:09,219 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2017-08-28 14:19:09,221 : INFO : downsampling leaves estimated 418 word corpus (27.2% of prior 1538)\n",
      "2017-08-28 14:19:09,225 : INFO : estimated required memory for 51 words and 100 dimensions: 34759105900 bytes\n",
      "2017-08-28 14:19:09,228 : INFO : Loading a fresh vocabulary\n",
      "2017-08-28 14:19:09,229 : INFO : min_count=17 retains 47 unique words (10% of original 444, drops 397)\n",
      "2017-08-28 14:19:09,231 : INFO : min_count=17 leaves 1474 word corpus (51% of original 2838, drops 1364)\n",
      "2017-08-28 14:19:09,234 : INFO : sample=0.001 downsamples 47 most-common words\n",
      "2017-08-28 14:19:09,237 : INFO : downsampling leaves estimated 382 word corpus (25.9% of prior 1474)\n",
      "2017-08-28 14:19:09,240 : INFO : estimated required memory for 47 words and 100 dimensions: 34759100700 bytes\n",
      "2017-08-28 14:19:09,243 : INFO : Loading a fresh vocabulary\n",
      "2017-08-28 14:19:09,245 : INFO : min_count=18 retains 43 unique words (9% of original 444, drops 401)\n",
      "2017-08-28 14:19:09,248 : INFO : min_count=18 leaves 1406 word corpus (49% of original 2838, drops 1432)\n",
      "2017-08-28 14:19:09,250 : INFO : sample=0.001 downsamples 43 most-common words\n",
      "2017-08-28 14:19:09,252 : INFO : downsampling leaves estimated 346 word corpus (24.7% of prior 1406)\n",
      "2017-08-28 14:19:09,256 : INFO : estimated required memory for 43 words and 100 dimensions: 34759095500 bytes\n",
      "2017-08-28 14:19:09,259 : INFO : Loading a fresh vocabulary\n",
      "2017-08-28 14:19:09,261 : INFO : min_count=19 retains 39 unique words (8% of original 444, drops 405)\n",
      "2017-08-28 14:19:09,263 : INFO : min_count=19 leaves 1334 word corpus (47% of original 2838, drops 1504)\n",
      "2017-08-28 14:19:09,265 : INFO : sample=0.001 downsamples 39 most-common words\n",
      "2017-08-28 14:19:09,270 : INFO : downsampling leaves estimated 311 word corpus (23.3% of prior 1334)\n",
      "2017-08-28 14:19:09,272 : INFO : estimated required memory for 39 words and 100 dimensions: 34759090300 bytes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_count: 13, size of vocab:  45.714285714285715\n",
      "min_count: 14, size of vocab:  44.285714285714285\n",
      "min_count: 15, size of vocab:  40.0\n",
      "min_count: 16, size of vocab:  36.42857142857143\n",
      "min_count: 17, size of vocab:  33.57142857142857\n",
      "min_count: 18, size of vocab:  30.714285714285715\n",
      "min_count: 19, size of vocab:  27.857142857142858\n"
     ]
    }
   ],
   "source": [
    "for num in range(0, 20):\n",
    "    print('min_count: {}, size of vocab: '.format(num), pre.scale_vocab(min_count=num, dry_run=True)['memory']['vocab']/700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Training the Doc2vec Model\n",
    "##PV-DBOW: paragraph vector - Distributed Bag of Words\n",
    "##PV-DM: paragraph vector - Distributed Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "# try both PV-DBOW and PV-DM model\n",
    "\n",
    "models = [\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, dbow_words=1, size=300, window=10, min_count=10, workers=cores),\n",
    "    # PV-DM w/average\n",
    "    Doc2Vec(dm=1, dm_mean=1, size=300, window=10, min_count=10, workers=cores),\n",
    "]\n",
    "\n",
    "#model = Doc2Vec(alpha=0.025, min_alpha=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-28 14:26:56,477 : INFO : collecting all words and their counts\n",
      "2017-08-28 14:26:56,479 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2017-08-28 14:26:56,482 : INFO : collected 444 word types and 86897599 unique tags from a corpus of 9 examples and 2838 words\n",
      "2017-08-28 14:26:56,484 : INFO : Loading a fresh vocabulary\n",
      "2017-08-28 14:26:56,488 : INFO : min_count=10 retains 81 unique words (18% of original 444, drops 363)\n",
      "2017-08-28 14:26:56,490 : INFO : min_count=10 leaves 1907 word corpus (67% of original 2838, drops 931)\n",
      "2017-08-28 14:26:56,493 : INFO : deleting the raw counts dictionary of 444 items\n",
      "2017-08-28 14:26:56,497 : INFO : sample=0.001 downsamples 81 most-common words\n",
      "2017-08-28 14:26:56,499 : INFO : downsampling leaves estimated 677 word corpus (35.5% of prior 1907)\n",
      "2017-08-28 14:26:56,501 : INFO : estimated required memory for 81 words and 300 dimensions: 104277353700 bytes\n",
      "2017-08-28 14:26:56,504 : INFO : resetting layer weights\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-f947f81ea4c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#model.build_vocab(patent_docs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatent_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[1;34m(self, sentences, keep_raw_vocab, trim_rule, progress_per, update)\u001b[0m\n\u001b[0;32m    551\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# initial survey\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# trim by min_count & precalculate downsampling\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinalize_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# build tables & arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mscan_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mfinalize_vocab\u001b[1;34m(self, update)\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[1;31m# set initial input/projection and hidden weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 726\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    727\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mreset_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    660\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"using concatenative %d-dimensional layer1\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer1_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 662\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    663\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    664\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mreset_weights\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    388\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoctag_syn0_lockf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoctag_syn0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoctag_syn0_lockf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# zeros suppress learning\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#model.build_vocab(patent_docs)\n",
    "models[0].build_vocab(patent_docs)\n",
    "print(str(models[0]))\n",
    "models[1].reset_from(models[0])\n",
    "print(str(models[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32mc:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m(390)\u001b[0;36mreset_weights\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m    388 \u001b[1;33m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoctag_syn0_lockf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    389 \u001b[1;33m        \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m--> 390 \u001b[1;33m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoctag_syn0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    391 \u001b[1;33m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoctag_syn0_lockf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# zeros suppress learning\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    392 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> print(self.doctag_syn0)\n",
      "*** AttributeError: 'DocvecsArray' object has no attribute 'doctag_syn0'\n",
      "ipdb> print(self)\n",
      "<gensim.models.doc2vec.DocvecsArray object at 0x0000024598ABB4E0>\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    %%time model.train(patent_docs, total_examples=model.corpus_count, epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-24 16:03:54,103 : INFO : training model with 4 workers on 130707 vocabulary and 200 features, using sg=1 hs=0 sample=0.001 negative=5 window=8\n",
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 837, in worker_loop\n",
      "    tally, raw_tally = self._do_train_job(sentences, alpha, (work, neu1))\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\doc2vec.py\", line 714, in _do_train_job\n",
      "    indexed_doctags = self.docvecs.indexed_doctags(doc.tags)\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\doc2vec.py\", line 313, in indexed_doctags\n",
      "    self.doctag_syn0, self.doctag_syn0_lockf, doctag_tokens)\n",
      "AttributeError: 'DocvecsArray' object has no attribute 'doctag_syn0'\n",
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 837, in worker_loop\n",
      "    tally, raw_tally = self._do_train_job(sentences, alpha, (work, neu1))\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\doc2vec.py\", line 714, in _do_train_job\n",
      "    indexed_doctags = self.docvecs.indexed_doctags(doc.tags)\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\doc2vec.py\", line 313, in indexed_doctags\n",
      "    self.doctag_syn0, self.doctag_syn0_lockf, doctag_tokens)\n",
      "AttributeError: 'DocvecsArray' object has no attribute 'doctag_syn0'\n",
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 837, in worker_loop\n",
      "    tally, raw_tally = self._do_train_job(sentences, alpha, (work, neu1))\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\doc2vec.py\", line 714, in _do_train_job\n",
      "    indexed_doctags = self.docvecs.indexed_doctags(doc.tags)\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\doc2vec.py\", line 313, in indexed_doctags\n",
      "    self.doctag_syn0, self.doctag_syn0_lockf, doctag_tokens)\n",
      "AttributeError: 'DocvecsArray' object has no attribute 'doctag_syn0'\n",
      "Exception in thread Thread-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 837, in worker_loop\n",
      "    tally, raw_tally = self._do_train_job(sentences, alpha, (work, neu1))\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\doc2vec.py\", line 714, in _do_train_job\n",
      "    indexed_doctags = self.docvecs.indexed_doctags(doc.tags)\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\doc2vec.py\", line 313, in indexed_doctags\n",
      "    self.doctag_syn0, self.doctag_syn0_lockf, doctag_tokens)\n",
      "AttributeError: 'DocvecsArray' object has no attribute 'doctag_syn0'\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train(patent_docs)\n",
    "    model.alpha -= 0.002\n",
    "    model.min_alpha = model.alpha\n",
    "    #%%time model.train(patent_docs, total_examples=model.corpus_count, epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('D2V')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class patentSentence:\n",
    "    def __init__(self, patent_doc):\n",
    "        self.patent_doc = patent_doc\n",
    "    def __iter__(self):\n",
    "        for pid, claims in self.patent_doc.items():\n",
    "            yield claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = patentSentence(patent_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-25 13:47:45,784 : INFO : collecting all words and their counts\n",
      "2017-08-25 13:47:45,790 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-08-25 13:47:55,228 : INFO : PROGRESS: at sentence #10000, processed 11311126 words, keeping 96531 word types\n",
      "2017-08-25 13:48:00,821 : INFO : PROGRESS: at sentence #20000, processed 22423771 words, keeping 148596 word types\n",
      "2017-08-25 13:48:05,039 : INFO : PROGRESS: at sentence #30000, processed 33935598 words, keeping 190634 word types\n",
      "2017-08-25 13:48:09,175 : INFO : PROGRESS: at sentence #40000, processed 45887431 words, keeping 234732 word types\n",
      "2017-08-25 13:48:12,427 : INFO : PROGRESS: at sentence #50000, processed 56519952 words, keeping 271546 word types\n",
      "2017-08-25 13:48:14,900 : INFO : collected 294968 word types from a corpus of 64957171 raw words and 58860 sentences\n",
      "2017-08-25 13:48:14,900 : INFO : Loading a fresh vocabulary\n",
      "2017-08-25 13:48:21,587 : INFO : min_count=0 retains 294968 unique words (100% of original 294968, drops 0)\n",
      "2017-08-25 13:48:21,587 : INFO : min_count=0 leaves 64957171 word corpus (100% of original 64957171, drops 0)\n",
      "2017-08-25 13:48:23,023 : INFO : deleting the raw counts dictionary of 294968 items\n",
      "2017-08-25 13:48:23,108 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2017-08-25 13:48:23,114 : INFO : downsampling leaves estimated 53401455 word corpus (82.2% of prior 64957171)\n",
      "2017-08-25 13:48:23,152 : INFO : estimated required memory for 294968 words and 100 dimensions: 383458400 bytes\n",
      "2017-08-25 13:48:25,223 : INFO : resetting layer weights\n",
      "2017-08-25 13:48:30,170 : INFO : training model with 3 workers on 294968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-08-25 13:48:31,539 : INFO : PROGRESS: at 0.24% examples, 586409 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:48:32,538 : INFO : PROGRESS: at 0.47% examples, 580830 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:48:33,563 : INFO : PROGRESS: at 0.70% examples, 623996 words/s, in_qsize 6, out_qsize 1\n",
      "2017-08-25 13:48:34,555 : INFO : PROGRESS: at 0.96% examples, 651545 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:48:35,576 : INFO : PROGRESS: at 1.22% examples, 670561 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:48:36,598 : INFO : PROGRESS: at 1.48% examples, 681919 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:48:37,598 : INFO : PROGRESS: at 1.75% examples, 692526 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:48:38,596 : INFO : PROGRESS: at 2.01% examples, 695823 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:48:39,614 : INFO : PROGRESS: at 2.28% examples, 700575 words/s, in_qsize 4, out_qsize 1\n",
      "2017-08-25 13:48:40,617 : INFO : PROGRESS: at 2.56% examples, 705192 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:48:41,628 : INFO : PROGRESS: at 2.83% examples, 707787 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:48:42,624 : INFO : PROGRESS: at 3.12% examples, 711773 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:48:43,635 : INFO : PROGRESS: at 3.41% examples, 713441 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:48:44,642 : INFO : PROGRESS: at 3.67% examples, 713931 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:48:45,650 : INFO : PROGRESS: at 3.91% examples, 708445 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:48:46,646 : INFO : PROGRESS: at 4.15% examples, 700288 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:48:47,648 : INFO : PROGRESS: at 4.45% examples, 702983 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:48:48,667 : INFO : PROGRESS: at 4.71% examples, 706091 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:48:49,666 : INFO : PROGRESS: at 4.99% examples, 711080 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:48:50,685 : INFO : PROGRESS: at 5.31% examples, 715618 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:48:51,705 : INFO : PROGRESS: at 5.59% examples, 718648 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:48:52,714 : INFO : PROGRESS: at 5.86% examples, 722621 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:48:53,730 : INFO : PROGRESS: at 6.21% examples, 726056 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:48:54,723 : INFO : PROGRESS: at 6.52% examples, 729354 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:48:55,730 : INFO : PROGRESS: at 6.81% examples, 732264 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:48:56,729 : INFO : PROGRESS: at 7.13% examples, 734988 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:48:57,746 : INFO : PROGRESS: at 7.43% examples, 737233 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:48:58,747 : INFO : PROGRESS: at 7.72% examples, 736844 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:48:59,747 : INFO : PROGRESS: at 7.99% examples, 735679 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:00,764 : INFO : PROGRESS: at 8.29% examples, 735698 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:01,780 : INFO : PROGRESS: at 8.58% examples, 737472 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:02,796 : INFO : PROGRESS: at 8.86% examples, 738552 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:03,805 : INFO : PROGRESS: at 9.12% examples, 738930 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:49:04,813 : INFO : PROGRESS: at 9.37% examples, 739861 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:05,813 : INFO : PROGRESS: at 9.59% examples, 738124 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:06,808 : INFO : PROGRESS: at 9.85% examples, 738213 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:07,835 : INFO : PROGRESS: at 10.10% examples, 738686 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:08,839 : INFO : PROGRESS: at 10.37% examples, 739546 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:09,861 : INFO : PROGRESS: at 10.63% examples, 740281 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:10,876 : INFO : PROGRESS: at 10.90% examples, 740859 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:11,890 : INFO : PROGRESS: at 11.15% examples, 741317 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:49:12,899 : INFO : PROGRESS: at 11.41% examples, 741776 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:13,912 : INFO : PROGRESS: at 11.70% examples, 742410 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:14,931 : INFO : PROGRESS: at 11.95% examples, 742357 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:49:15,924 : INFO : PROGRESS: at 12.18% examples, 741851 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:16,932 : INFO : PROGRESS: at 12.37% examples, 737923 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:17,958 : INFO : PROGRESS: at 12.59% examples, 734584 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:49:18,952 : INFO : PROGRESS: at 12.84% examples, 734576 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:20,001 : INFO : PROGRESS: at 13.09% examples, 733007 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:21,004 : INFO : PROGRESS: at 13.30% examples, 729651 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:21,990 : INFO : PROGRESS: at 13.50% examples, 726831 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:23,006 : INFO : PROGRESS: at 13.77% examples, 727366 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:24,020 : INFO : PROGRESS: at 14.01% examples, 728075 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:25,030 : INFO : PROGRESS: at 14.27% examples, 728073 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:26,033 : INFO : PROGRESS: at 14.49% examples, 728348 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:27,067 : INFO : PROGRESS: at 14.72% examples, 726525 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:49:28,067 : INFO : PROGRESS: at 14.90% examples, 723134 words/s, in_qsize 5, out_qsize 1\n",
      "2017-08-25 13:49:29,068 : INFO : PROGRESS: at 15.13% examples, 721558 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:30,081 : INFO : PROGRESS: at 15.39% examples, 721677 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:31,085 : INFO : PROGRESS: at 15.71% examples, 722208 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:32,099 : INFO : PROGRESS: at 16.00% examples, 721312 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:49:33,091 : INFO : PROGRESS: at 16.27% examples, 718593 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:34,112 : INFO : PROGRESS: at 16.62% examples, 718608 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:35,115 : INFO : PROGRESS: at 16.94% examples, 717673 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-25 13:49:36,133 : INFO : PROGRESS: at 17.30% examples, 717875 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:49:37,138 : INFO : PROGRESS: at 17.59% examples, 718161 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:38,157 : INFO : PROGRESS: at 17.91% examples, 718664 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:39,173 : INFO : PROGRESS: at 18.22% examples, 718834 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:49:40,163 : INFO : PROGRESS: at 18.55% examples, 719232 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:41,166 : INFO : PROGRESS: at 18.88% examples, 719904 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:42,174 : INFO : PROGRESS: at 19.23% examples, 720651 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:43,174 : INFO : PROGRESS: at 19.59% examples, 720838 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:44,178 : INFO : PROGRESS: at 19.90% examples, 721015 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:45,182 : INFO : PROGRESS: at 20.21% examples, 721075 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:46,216 : INFO : PROGRESS: at 20.49% examples, 721332 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:47,220 : INFO : PROGRESS: at 20.71% examples, 720611 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:48,240 : INFO : PROGRESS: at 20.95% examples, 719763 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:49:49,250 : INFO : PROGRESS: at 21.21% examples, 720506 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:50,261 : INFO : PROGRESS: at 21.48% examples, 721121 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:51,255 : INFO : PROGRESS: at 21.75% examples, 721689 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:52,270 : INFO : PROGRESS: at 22.03% examples, 722385 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:53,287 : INFO : PROGRESS: at 22.31% examples, 722831 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:54,276 : INFO : PROGRESS: at 22.61% examples, 723470 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:55,294 : INFO : PROGRESS: at 22.90% examples, 724077 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:49:56,298 : INFO : PROGRESS: at 23.19% examples, 724657 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:57,302 : INFO : PROGRESS: at 23.48% examples, 725109 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:58,306 : INFO : PROGRESS: at 23.78% examples, 725741 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:49:59,326 : INFO : PROGRESS: at 24.09% examples, 726176 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:00,326 : INFO : PROGRESS: at 24.39% examples, 726554 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:50:01,330 : INFO : PROGRESS: at 24.65% examples, 726840 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:50:02,337 : INFO : PROGRESS: at 24.93% examples, 727297 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:03,354 : INFO : PROGRESS: at 25.21% examples, 727677 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:04,345 : INFO : PROGRESS: at 25.50% examples, 728076 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:05,362 : INFO : PROGRESS: at 25.76% examples, 728432 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:06,366 : INFO : PROGRESS: at 26.08% examples, 728836 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:07,368 : INFO : PROGRESS: at 26.36% examples, 729069 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:08,374 : INFO : PROGRESS: at 26.66% examples, 729366 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:09,379 : INFO : PROGRESS: at 26.93% examples, 729608 words/s, in_qsize 6, out_qsize 1\n",
      "2017-08-25 13:50:10,378 : INFO : PROGRESS: at 27.23% examples, 729952 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:11,381 : INFO : PROGRESS: at 27.53% examples, 730379 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:12,401 : INFO : PROGRESS: at 27.84% examples, 730785 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:50:13,404 : INFO : PROGRESS: at 28.14% examples, 730992 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:50:14,408 : INFO : PROGRESS: at 28.42% examples, 731394 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:50:15,398 : INFO : PROGRESS: at 28.71% examples, 731627 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:16,414 : INFO : PROGRESS: at 28.97% examples, 732060 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:17,419 : INFO : PROGRESS: at 29.23% examples, 732395 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:18,423 : INFO : PROGRESS: at 29.47% examples, 732718 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:19,448 : INFO : PROGRESS: at 29.75% examples, 733081 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:20,444 : INFO : PROGRESS: at 29.99% examples, 733185 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:50:21,447 : INFO : PROGRESS: at 30.23% examples, 732902 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:22,467 : INFO : PROGRESS: at 30.46% examples, 732366 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:50:23,464 : INFO : PROGRESS: at 30.70% examples, 732311 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:24,475 : INFO : PROGRESS: at 30.97% examples, 732535 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:50:25,478 : INFO : PROGRESS: at 31.23% examples, 732760 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:26,497 : INFO : PROGRESS: at 31.50% examples, 733018 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:27,515 : INFO : PROGRESS: at 31.77% examples, 733364 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:50:28,506 : INFO : PROGRESS: at 32.03% examples, 733669 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:29,533 : INFO : PROGRESS: at 32.28% examples, 733821 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:30,541 : INFO : PROGRESS: at 32.54% examples, 733862 words/s, in_qsize 6, out_qsize 1\n",
      "2017-08-25 13:50:31,531 : INFO : PROGRESS: at 32.79% examples, 733518 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:32,535 : INFO : PROGRESS: at 33.04% examples, 733238 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:33,553 : INFO : PROGRESS: at 33.31% examples, 733350 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:34,557 : INFO : PROGRESS: at 33.54% examples, 732757 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:50:35,560 : INFO : PROGRESS: at 33.79% examples, 732520 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:36,580 : INFO : PROGRESS: at 34.01% examples, 732396 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:37,584 : INFO : PROGRESS: at 34.29% examples, 732730 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:38,587 : INFO : PROGRESS: at 34.53% examples, 733031 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:39,606 : INFO : PROGRESS: at 34.80% examples, 733265 words/s, in_qsize 6, out_qsize 1\n",
      "2017-08-25 13:50:40,608 : INFO : PROGRESS: at 35.07% examples, 733547 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:41,612 : INFO : PROGRESS: at 35.34% examples, 733808 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:42,639 : INFO : PROGRESS: at 35.66% examples, 734038 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:50:43,634 : INFO : PROGRESS: at 35.98% examples, 733953 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:44,638 : INFO : PROGRESS: at 36.31% examples, 733662 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:45,643 : INFO : PROGRESS: at 36.65% examples, 733655 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:46,646 : INFO : PROGRESS: at 37.03% examples, 733774 words/s, in_qsize 4, out_qsize 1\n",
      "2017-08-25 13:50:47,650 : INFO : PROGRESS: at 37.38% examples, 734040 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:50:48,661 : INFO : PROGRESS: at 37.63% examples, 733313 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:49,666 : INFO : PROGRESS: at 37.92% examples, 732938 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:50,660 : INFO : PROGRESS: at 38.25% examples, 733158 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:51,665 : INFO : PROGRESS: at 38.58% examples, 733421 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:52,681 : INFO : PROGRESS: at 38.92% examples, 733573 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:53,706 : INFO : PROGRESS: at 39.26% examples, 733695 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:54,705 : INFO : PROGRESS: at 39.63% examples, 733912 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:55,709 : INFO : PROGRESS: at 39.96% examples, 734162 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-25 13:50:56,713 : INFO : PROGRESS: at 40.27% examples, 734350 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:57,733 : INFO : PROGRESS: at 40.56% examples, 734519 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:50:58,751 : INFO : PROGRESS: at 40.80% examples, 734556 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:50:59,755 : INFO : PROGRESS: at 41.08% examples, 734886 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:00,787 : INFO : PROGRESS: at 41.34% examples, 735070 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:01,792 : INFO : PROGRESS: at 41.62% examples, 735296 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:02,796 : INFO : PROGRESS: at 41.89% examples, 735571 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:03,815 : INFO : PROGRESS: at 42.17% examples, 735715 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:04,819 : INFO : PROGRESS: at 42.46% examples, 735954 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:05,828 : INFO : PROGRESS: at 42.75% examples, 736184 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:06,827 : INFO : PROGRESS: at 43.03% examples, 736363 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:07,830 : INFO : PROGRESS: at 43.34% examples, 736627 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:08,834 : INFO : PROGRESS: at 43.62% examples, 736786 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:09,852 : INFO : PROGRESS: at 43.92% examples, 736963 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:10,858 : INFO : PROGRESS: at 44.23% examples, 737248 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:11,859 : INFO : PROGRESS: at 44.54% examples, 737498 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:12,878 : INFO : PROGRESS: at 44.80% examples, 737667 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:13,887 : INFO : PROGRESS: at 45.07% examples, 737846 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:51:14,902 : INFO : PROGRESS: at 45.37% examples, 738006 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:51:15,912 : INFO : PROGRESS: at 45.64% examples, 738244 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:16,908 : INFO : PROGRESS: at 45.91% examples, 738459 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:17,912 : INFO : PROGRESS: at 46.24% examples, 738544 words/s, in_qsize 4, out_qsize 1\n",
      "2017-08-25 13:51:18,915 : INFO : PROGRESS: at 46.54% examples, 738759 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:19,920 : INFO : PROGRESS: at 46.82% examples, 738978 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:20,942 : INFO : PROGRESS: at 47.13% examples, 739167 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:51:21,955 : INFO : PROGRESS: at 47.41% examples, 739238 words/s, in_qsize 4, out_qsize 1\n",
      "2017-08-25 13:51:22,944 : INFO : PROGRESS: at 47.72% examples, 739472 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:23,963 : INFO : PROGRESS: at 48.02% examples, 739549 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:24,967 : INFO : PROGRESS: at 48.32% examples, 739763 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:25,970 : INFO : PROGRESS: at 48.61% examples, 739937 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:26,990 : INFO : PROGRESS: at 48.89% examples, 740099 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:27,994 : INFO : PROGRESS: at 49.14% examples, 740306 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:28,997 : INFO : PROGRESS: at 49.40% examples, 740475 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:51:30,001 : INFO : PROGRESS: at 49.65% examples, 740506 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:31,013 : INFO : PROGRESS: at 49.91% examples, 740698 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:32,006 : INFO : PROGRESS: at 50.16% examples, 740860 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:33,009 : INFO : PROGRESS: at 50.43% examples, 741018 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:34,030 : INFO : PROGRESS: at 50.68% examples, 740973 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:51:35,041 : INFO : PROGRESS: at 50.91% examples, 740453 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:51:36,052 : INFO : PROGRESS: at 51.16% examples, 740464 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:51:37,072 : INFO : PROGRESS: at 51.42% examples, 740553 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:38,076 : INFO : PROGRESS: at 51.70% examples, 740732 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:39,080 : INFO : PROGRESS: at 51.97% examples, 740906 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:40,083 : INFO : PROGRESS: at 52.21% examples, 740998 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:41,103 : INFO : PROGRESS: at 52.47% examples, 741166 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:51:42,104 : INFO : PROGRESS: at 52.76% examples, 741318 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:51:43,108 : INFO : PROGRESS: at 53.02% examples, 741382 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:44,111 : INFO : PROGRESS: at 53.30% examples, 741361 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:45,130 : INFO : PROGRESS: at 53.54% examples, 741173 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:46,150 : INFO : PROGRESS: at 53.81% examples, 741200 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:47,163 : INFO : PROGRESS: at 54.06% examples, 741406 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:48,174 : INFO : PROGRESS: at 54.33% examples, 741586 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:49,178 : INFO : PROGRESS: at 54.56% examples, 741599 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:50,181 : INFO : PROGRESS: at 54.77% examples, 740725 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:51,200 : INFO : PROGRESS: at 55.05% examples, 740947 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:51:52,212 : INFO : PROGRESS: at 55.31% examples, 741105 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:51:53,219 : INFO : PROGRESS: at 55.62% examples, 741241 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:54,210 : INFO : PROGRESS: at 55.96% examples, 741329 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:55,229 : INFO : PROGRESS: at 56.32% examples, 741427 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:56,232 : INFO : PROGRESS: at 56.68% examples, 741531 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:57,228 : INFO : PROGRESS: at 57.06% examples, 741582 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:58,240 : INFO : PROGRESS: at 57.41% examples, 741689 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:51:59,243 : INFO : PROGRESS: at 57.73% examples, 741871 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:00,263 : INFO : PROGRESS: at 58.05% examples, 741997 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:01,266 : INFO : PROGRESS: at 58.38% examples, 742101 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:02,275 : INFO : PROGRESS: at 58.71% examples, 742204 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:03,273 : INFO : PROGRESS: at 59.05% examples, 742354 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:04,277 : INFO : PROGRESS: at 59.41% examples, 742463 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:05,280 : INFO : PROGRESS: at 59.76% examples, 742554 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:06,273 : INFO : PROGRESS: at 60.08% examples, 742646 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:07,292 : INFO : PROGRESS: at 60.38% examples, 742756 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:08,291 : INFO : PROGRESS: at 60.65% examples, 742938 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:09,294 : INFO : PROGRESS: at 60.93% examples, 743113 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:10,298 : INFO : PROGRESS: at 61.19% examples, 743271 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:11,302 : INFO : PROGRESS: at 61.45% examples, 743303 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:12,304 : INFO : PROGRESS: at 61.73% examples, 743419 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:13,309 : INFO : PROGRESS: at 62.01% examples, 743612 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:14,312 : INFO : PROGRESS: at 62.28% examples, 743720 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:15,330 : INFO : PROGRESS: at 62.58% examples, 743783 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:16,350 : INFO : PROGRESS: at 62.86% examples, 743885 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-25 13:52:17,353 : INFO : PROGRESS: at 63.12% examples, 743587 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:18,373 : INFO : PROGRESS: at 63.40% examples, 743441 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:19,374 : INFO : PROGRESS: at 63.69% examples, 743549 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:52:20,381 : INFO : PROGRESS: at 63.98% examples, 743617 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:21,383 : INFO : PROGRESS: at 64.28% examples, 743705 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:52:22,383 : INFO : PROGRESS: at 64.58% examples, 743832 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:23,390 : INFO : PROGRESS: at 64.85% examples, 743930 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:24,395 : INFO : PROGRESS: at 65.12% examples, 744051 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:25,400 : INFO : PROGRESS: at 65.42% examples, 744090 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:26,432 : INFO : PROGRESS: at 65.66% examples, 743933 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:27,423 : INFO : PROGRESS: at 65.93% examples, 743934 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:28,444 : INFO : PROGRESS: at 66.26% examples, 744029 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:29,443 : INFO : PROGRESS: at 66.54% examples, 744067 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:30,446 : INFO : PROGRESS: at 66.82% examples, 744138 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:52:31,451 : INFO : PROGRESS: at 67.13% examples, 744251 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:32,455 : INFO : PROGRESS: at 67.42% examples, 744368 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:33,472 : INFO : PROGRESS: at 67.73% examples, 744433 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:34,480 : INFO : PROGRESS: at 68.03% examples, 744533 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:35,495 : INFO : PROGRESS: at 68.32% examples, 744539 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:36,498 : INFO : PROGRESS: at 68.61% examples, 744660 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:37,518 : INFO : PROGRESS: at 68.87% examples, 744538 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:38,521 : INFO : PROGRESS: at 69.13% examples, 744636 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:39,526 : INFO : PROGRESS: at 69.38% examples, 744700 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:40,529 : INFO : PROGRESS: at 69.64% examples, 744843 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:41,532 : INFO : PROGRESS: at 69.90% examples, 744957 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:42,539 : INFO : PROGRESS: at 70.15% examples, 745042 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:43,538 : INFO : PROGRESS: at 70.40% examples, 744902 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:44,542 : INFO : PROGRESS: at 70.62% examples, 744486 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:45,530 : INFO : PROGRESS: at 70.82% examples, 744019 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:46,540 : INFO : PROGRESS: at 71.05% examples, 743618 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:47,569 : INFO : PROGRESS: at 71.27% examples, 743252 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:48,588 : INFO : PROGRESS: at 71.50% examples, 742865 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:49,591 : INFO : PROGRESS: at 71.74% examples, 742654 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:50,597 : INFO : PROGRESS: at 71.97% examples, 742430 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:51,599 : INFO : PROGRESS: at 72.18% examples, 742049 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:52,610 : INFO : PROGRESS: at 72.44% examples, 742158 words/s, in_qsize 6, out_qsize 1\n",
      "2017-08-25 13:52:53,603 : INFO : PROGRESS: at 72.73% examples, 742288 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:54,609 : INFO : PROGRESS: at 73.00% examples, 742343 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:55,628 : INFO : PROGRESS: at 73.27% examples, 742359 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:56,630 : INFO : PROGRESS: at 73.55% examples, 742533 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:57,647 : INFO : PROGRESS: at 73.82% examples, 742618 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:58,654 : INFO : PROGRESS: at 74.06% examples, 742701 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:52:59,674 : INFO : PROGRESS: at 74.33% examples, 742807 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:53:00,677 : INFO : PROGRESS: at 74.57% examples, 742922 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:01,682 : INFO : PROGRESS: at 74.84% examples, 743014 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:53:02,694 : INFO : PROGRESS: at 75.13% examples, 743122 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:03,716 : INFO : PROGRESS: at 75.40% examples, 743217 words/s, in_qsize 4, out_qsize 1\n",
      "2017-08-25 13:53:04,721 : INFO : PROGRESS: at 75.73% examples, 743346 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:05,725 : INFO : PROGRESS: at 76.06% examples, 743381 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:06,729 : INFO : PROGRESS: at 76.43% examples, 743434 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:07,748 : INFO : PROGRESS: at 76.80% examples, 743510 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:08,747 : INFO : PROGRESS: at 77.18% examples, 743564 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:09,772 : INFO : PROGRESS: at 77.50% examples, 743653 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:53:10,774 : INFO : PROGRESS: at 77.83% examples, 743757 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:11,778 : INFO : PROGRESS: at 78.14% examples, 743807 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:12,787 : INFO : PROGRESS: at 78.47% examples, 743879 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:13,785 : INFO : PROGRESS: at 78.81% examples, 743985 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:14,787 : INFO : PROGRESS: at 79.16% examples, 744041 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:15,808 : INFO : PROGRESS: at 79.52% examples, 744081 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:16,808 : INFO : PROGRESS: at 79.85% examples, 744142 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:17,815 : INFO : PROGRESS: at 80.18% examples, 744222 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:18,818 : INFO : PROGRESS: at 80.48% examples, 744309 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:19,838 : INFO : PROGRESS: at 80.72% examples, 744378 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:20,842 : INFO : PROGRESS: at 81.01% examples, 744510 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:21,845 : INFO : PROGRESS: at 81.26% examples, 744568 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:22,858 : INFO : PROGRESS: at 81.53% examples, 744653 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:53:23,868 : INFO : PROGRESS: at 81.80% examples, 744739 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:24,881 : INFO : PROGRESS: at 82.09% examples, 744812 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:53:25,874 : INFO : PROGRESS: at 82.37% examples, 744842 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:26,880 : INFO : PROGRESS: at 82.65% examples, 744941 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:27,884 : INFO : PROGRESS: at 82.94% examples, 745019 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:28,884 : INFO : PROGRESS: at 83.23% examples, 745083 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:29,888 : INFO : PROGRESS: at 83.52% examples, 745130 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:30,892 : INFO : PROGRESS: at 83.81% examples, 745170 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:31,926 : INFO : PROGRESS: at 84.11% examples, 745134 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:32,931 : INFO : PROGRESS: at 84.36% examples, 744727 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:33,933 : INFO : PROGRESS: at 84.59% examples, 744346 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:34,955 : INFO : PROGRESS: at 84.77% examples, 743605 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:53:35,955 : INFO : PROGRESS: at 85.00% examples, 743308 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:53:36,963 : INFO : PROGRESS: at 85.24% examples, 742982 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-25 13:53:37,968 : INFO : PROGRESS: at 85.45% examples, 742344 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:38,972 : INFO : PROGRESS: at 85.65% examples, 741816 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:53:39,964 : INFO : PROGRESS: at 85.84% examples, 741208 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:40,983 : INFO : PROGRESS: at 86.11% examples, 740755 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:41,987 : INFO : PROGRESS: at 86.34% examples, 740321 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:53:42,990 : INFO : PROGRESS: at 86.58% examples, 739963 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:43,992 : INFO : PROGRESS: at 86.82% examples, 739652 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:44,997 : INFO : PROGRESS: at 86.99% examples, 738788 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:46,002 : INFO : PROGRESS: at 87.22% examples, 738261 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:47,005 : INFO : PROGRESS: at 87.44% examples, 737742 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:48,035 : INFO : PROGRESS: at 87.67% examples, 737241 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:53:49,039 : INFO : PROGRESS: at 87.93% examples, 736872 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:53:50,052 : INFO : PROGRESS: at 88.16% examples, 736371 words/s, in_qsize 4, out_qsize 1\n",
      "2017-08-25 13:53:51,060 : INFO : PROGRESS: at 88.33% examples, 735489 words/s, in_qsize 6, out_qsize 1\n",
      "2017-08-25 13:53:52,055 : INFO : PROGRESS: at 88.51% examples, 734677 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:53:53,078 : INFO : PROGRESS: at 88.67% examples, 733753 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:54,084 : INFO : PROGRESS: at 88.85% examples, 733027 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:53:55,094 : INFO : PROGRESS: at 89.03% examples, 732406 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:53:56,103 : INFO : PROGRESS: at 89.19% examples, 731706 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:53:57,134 : INFO : PROGRESS: at 89.37% examples, 730958 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:53:58,137 : INFO : PROGRESS: at 89.51% examples, 730201 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:53:59,160 : INFO : PROGRESS: at 89.69% examples, 729373 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:54:00,177 : INFO : PROGRESS: at 89.85% examples, 728579 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:01,186 : INFO : PROGRESS: at 90.01% examples, 727931 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:02,186 : INFO : PROGRESS: at 90.15% examples, 727000 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:03,189 : INFO : PROGRESS: at 90.33% examples, 726378 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:54:04,200 : INFO : PROGRESS: at 90.54% examples, 725942 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:05,203 : INFO : PROGRESS: at 90.72% examples, 725472 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:54:06,204 : INFO : PROGRESS: at 90.95% examples, 725161 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:07,212 : INFO : PROGRESS: at 91.15% examples, 724865 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:08,239 : INFO : PROGRESS: at 91.35% examples, 724400 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:09,271 : INFO : PROGRESS: at 91.52% examples, 723614 words/s, in_qsize 6, out_qsize 1\n",
      "2017-08-25 13:54:10,273 : INFO : PROGRESS: at 91.70% examples, 722964 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:11,279 : INFO : PROGRESS: at 91.88% examples, 722359 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:54:12,288 : INFO : PROGRESS: at 92.08% examples, 722031 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:13,284 : INFO : PROGRESS: at 92.30% examples, 721852 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:14,294 : INFO : PROGRESS: at 92.57% examples, 721950 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:54:15,295 : INFO : PROGRESS: at 92.83% examples, 722098 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:16,295 : INFO : PROGRESS: at 93.12% examples, 722220 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:17,313 : INFO : PROGRESS: at 93.38% examples, 722226 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:18,321 : INFO : PROGRESS: at 93.61% examples, 722086 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:19,336 : INFO : PROGRESS: at 93.84% examples, 721846 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:20,345 : INFO : PROGRESS: at 94.08% examples, 721905 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:54:21,349 : INFO : PROGRESS: at 94.33% examples, 721936 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:22,366 : INFO : PROGRESS: at 94.57% examples, 722076 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:23,367 : INFO : PROGRESS: at 94.84% examples, 722126 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:24,368 : INFO : PROGRESS: at 95.11% examples, 722198 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:25,379 : INFO : PROGRESS: at 95.38% examples, 722317 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:26,391 : INFO : PROGRESS: at 95.70% examples, 722435 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:27,392 : INFO : PROGRESS: at 96.02% examples, 722509 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:28,411 : INFO : PROGRESS: at 96.39% examples, 722626 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:29,431 : INFO : PROGRESS: at 96.76% examples, 722695 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-25 13:54:30,441 : INFO : PROGRESS: at 97.14% examples, 722772 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:31,439 : INFO : PROGRESS: at 97.45% examples, 722783 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:32,454 : INFO : PROGRESS: at 97.77% examples, 722898 words/s, in_qsize 6, out_qsize 1\n",
      "2017-08-25 13:54:33,460 : INFO : PROGRESS: at 98.08% examples, 722964 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:34,480 : INFO : PROGRESS: at 98.39% examples, 722946 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:35,486 : INFO : PROGRESS: at 98.72% examples, 723002 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:36,494 : INFO : PROGRESS: at 99.05% examples, 723103 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:37,495 : INFO : PROGRESS: at 99.42% examples, 723211 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:38,527 : INFO : PROGRESS: at 99.76% examples, 723309 words/s, in_qsize 5, out_qsize 0\n",
      "2017-08-25 13:54:39,246 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-08-25 13:54:39,260 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-08-25 13:54:39,265 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-08-25 13:54:39,266 : INFO : training on 324785855 raw words (266737394 effective words) took 368.7s, 723386 effective words/s\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "word2vect_model = gensim.models.Word2Vec(sentences, min_count = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-25 13:56:40,054 : INFO : saving Word2Vec object under word2vecmodel, separately None\n",
      "2017-08-25 13:56:40,059 : INFO : not storing attribute syn0norm\n",
      "2017-08-25 13:56:40,062 : INFO : storing np array 'syn0' to word2vecmodel.wv.syn0.npy\n",
      "2017-08-25 13:56:40,720 : INFO : storing np array 'syn1neg' to word2vecmodel.syn1neg.npy\n",
      "2017-08-25 13:56:41,272 : INFO : not storing attribute cum_table\n",
      "2017-08-25 13:56:42,649 : INFO : saved word2vecmodel\n"
     ]
    }
   ],
   "source": [
    "word2vect_model.save(\"word2vecmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('emergency', 0.025825158),\n",
       " ('beacon', 0.0017108296),\n",
       " ('message', 0.0013486465),\n",
       " ('IVS', 0.0013114822),\n",
       " ('PPT', 0.0011884869),\n",
       " ('missed', 0.00047500365),\n",
       " ('DTIM', 0.0004336031),\n",
       " ('microcell', 0.00033761081),\n",
       " ('signaltransmitting', 0.00028154752),\n",
       " ('alert', 0.00027658624)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vect_model.predict_output_word(['emergency', 'beacon', 'received'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
