{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (7,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# read patent high level mapping and claims\n",
    "claims = pd.read_csv(\"../data/claims.csv\", encoding = 'utf8')\n",
    "patent_spec_map = pd.read_csv(\"../data/patent_spec_map.csv\", encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claim mapping has while patent_spec mapping don't percent:  0.46722731906218146\n",
      "patent_spec mapping has while claim mapping don't percent:  0.33871899422358137\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "print(\"claim mapping has while patent_spec mapping don't percent: \", len(set(claims[\"PATENT_ID\"]) - set(patent_spec_map[\"PATENT_ID\"])) / len(set(claims[\"PATENT_ID\"])))\n",
    "print(\"patent_spec mapping has while claim mapping don't percent: \", len(set(patent_spec_map[\"PATENT_ID\"]) - set(claims[\"PATENT_ID\"])) / len(set(claims[\"PATENT_ID\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intersection:  31359\n"
     ]
    }
   ],
   "source": [
    "print(\"intersection: \", len(set(claims[\"PATENT_ID\"]).intersection(set(patent_spec_map[\"PATENT_ID\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claim size:  58860\n",
      "patent size:  51296\n"
     ]
    }
   ],
   "source": [
    "print(\"claim size: \", len(set(claims[\"PATENT_ID\"])))\n",
    "print(\"patent size: \", len(set(patent_spec_map[\"PATENT_ID\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import tokenize\n",
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from pprint import pprint\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> import logging\n",
    ">>> logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process data into a document corpus and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_process(sent):\n",
    "    words = str(sent).split()\n",
    "\n",
    "    # 1. alphanumeric only\n",
    "    alphanum_only = [re.sub(\"[^a-zA-Z0-9]\", \"\", w) for w in words]\n",
    "\n",
    "    # 2. remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    meaningful_words = [w for w in alphanum_only if not w in stops]\n",
    "\n",
    "    # 3. lemmatization\n",
    "    words_lemma = [self.lemma.lemmatize(w) for w in meaningful_words]\n",
    "\n",
    "    # 6. Join the words back into one string separated by space,\n",
    "    # and return the result.\n",
    "    return( \" \".join( words_lemma ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patent_corpus = {}\n",
    "for i, v in claims.iterrows():\n",
    "    pid = v[\"PATENT_ID\"]\n",
    "    text = sentence_process(v[\"CLAIM_TEXT\"])\n",
    "    if(pid not in patent_corpus):\n",
    "        patent_corpus[pid] = text\n",
    "    else:\n",
    "        patent_corpus[pid].extend(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del claims\n",
    "del patent_spec_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Document Embedding class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TaggedPatentDocument:\n",
    "    def __init__(self, patent):\n",
    "        self.patent = patent\n",
    "    def __iter__(self):\n",
    "        for pid, content in self.patent.items():\n",
    "            yield TaggedDocument(content, [pid])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patent_docs = TaggedPatentDocument(patent_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling min_cout size for vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-24 13:46:47,075 : INFO : collecting all words and their counts\n",
      "2017-08-24 13:46:47,106 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2017-08-24 13:46:57,200 : INFO : PROGRESS: at example #10000, processed 16291689 words (1614301/s), 52638 word types, 163188390 tags\n",
      "2017-08-24 13:47:06,093 : INFO : PROGRESS: at example #20000, processed 32621419 words (1836190/s), 75605 word types, 163188390 tags\n",
      "2017-08-24 13:47:15,879 : INFO : PROGRESS: at example #30000, processed 49361312 words (1709612/s), 92632 word types, 163455185 tags\n",
      "2017-08-24 13:47:29,303 : INFO : PROGRESS: at example #40000, processed 66737559 words (1295223/s), 109277 word types, 163455185 tags\n",
      "2017-08-24 13:47:38,949 : INFO : PROGRESS: at example #50000, processed 82367850 words (1619441/s), 122903 word types, 167990131 tags\n",
      "2017-08-24 13:47:50,149 : INFO : collected 130707 word types and 167990131 unique tags from a corpus of 58860 examples and 95007318 words\n"
     ]
    }
   ],
   "source": [
    "pre = Doc2Vec(min_count=0)\n",
    "pre.scan_vocab(patent_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-24 13:47:50,212 : INFO : Loading a fresh vocabulary\n",
      "2017-08-24 13:47:50,748 : INFO : min_count=0 retains 130707 unique words (100% of original 130707, drops 0)\n",
      "2017-08-24 13:47:50,764 : INFO : min_count=0 leaves 95007318 word corpus (100% of original 95007318, drops 0)\n",
      "2017-08-24 13:47:51,182 : INFO : sample=0.001 downsamples 50 most-common words\n",
      "2017-08-24 13:47:51,182 : INFO : downsampling leaves estimated 65167521 word corpus (68.6% of prior 95007318)\n",
      "2017-08-24 13:47:51,235 : INFO : estimated required memory for 130707 words and 100 dimensions: 67365971500 bytes\n",
      "2017-08-24 13:47:51,258 : INFO : Loading a fresh vocabulary\n",
      "2017-08-24 13:47:51,414 : INFO : min_count=1 retains 130707 unique words (100% of original 130707, drops 0)\n",
      "2017-08-24 13:47:51,414 : INFO : min_count=1 leaves 95007318 word corpus (100% of original 95007318, drops 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_count: 0, size of vocab:  93362.14285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-24 13:47:51,833 : INFO : sample=0.001 downsamples 50 most-common words\n",
      "2017-08-24 13:47:51,849 : INFO : downsampling leaves estimated 65167521 word corpus (68.6% of prior 95007318)\n",
      "2017-08-24 13:47:51,849 : INFO : estimated required memory for 130707 words and 100 dimensions: 67365971500 bytes\n",
      "2017-08-24 13:47:51,859 : INFO : Loading a fresh vocabulary\n",
      "2017-08-24 13:47:51,982 : INFO : min_count=2 retains 91848 unique words (70% of original 130707, drops 38859)\n",
      "2017-08-24 13:47:51,986 : INFO : min_count=2 leaves 94968459 word corpus (99% of original 95007318, drops 38859)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_count: 1, size of vocab:  93362.14285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-24 13:47:52,277 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2017-08-24 13:47:52,293 : INFO : downsampling leaves estimated 65123942 word corpus (68.6% of prior 94968459)\n",
      "2017-08-24 13:47:52,293 : INFO : estimated required memory for 91848 words and 100 dimensions: 67315454800 bytes\n",
      "2017-08-24 13:47:52,303 : INFO : Loading a fresh vocabulary\n",
      "2017-08-24 13:47:52,401 : INFO : min_count=3 retains 74627 unique words (57% of original 130707, drops 56080)\n",
      "2017-08-24 13:47:52,401 : INFO : min_count=3 leaves 94934017 word corpus (99% of original 95007318, drops 73301)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_count: 2, size of vocab:  65605.71428571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-24 13:47:52,661 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2017-08-24 13:47:52,661 : INFO : downsampling leaves estimated 65085266 word corpus (68.6% of prior 94934017)\n",
      "2017-08-24 13:47:52,669 : INFO : estimated required memory for 74627 words and 100 dimensions: 67293067500 bytes\n",
      "2017-08-24 13:47:52,677 : INFO : Loading a fresh vocabulary\n",
      "2017-08-24 13:47:52,790 : INFO : min_count=4 retains 66346 unique words (50% of original 130707, drops 64361)\n",
      "2017-08-24 13:47:52,790 : INFO : min_count=4 leaves 94909174 word corpus (99% of original 95007318, drops 98144)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_count: 3, size of vocab:  53305.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-24 13:47:53,030 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2017-08-24 13:47:53,030 : INFO : downsampling leaves estimated 65057368 word corpus (68.5% of prior 94909174)\n",
      "2017-08-24 13:47:53,030 : INFO : estimated required memory for 66346 words and 100 dimensions: 67282302200 bytes\n",
      "2017-08-24 13:47:53,041 : INFO : Loading a fresh vocabulary\n",
      "2017-08-24 13:47:53,137 : INFO : min_count=5 retains 58423 unique words (44% of original 130707, drops 72284)\n",
      "2017-08-24 13:47:53,137 : INFO : min_count=5 leaves 94877482 word corpus (99% of original 95007318, drops 129836)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_count: 4, size of vocab:  47390.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-24 13:47:53,325 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2017-08-24 13:47:53,325 : INFO : downsampling leaves estimated 65021780 word corpus (68.5% of prior 94877482)\n",
      "2017-08-24 13:47:53,339 : INFO : estimated required memory for 58423 words and 100 dimensions: 67272002300 bytes\n",
      "2017-08-24 13:47:53,343 : INFO : Loading a fresh vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_count: 5, size of vocab:  41730.71428571428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-24 13:47:53,645 : INFO : min_count=6 retains 54370 unique words (41% of original 130707, drops 76337)\n",
      "2017-08-24 13:47:53,645 : INFO : min_count=6 leaves 94857217 word corpus (99% of original 95007318, drops 150101)\n",
      "2017-08-24 13:47:53,818 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2017-08-24 13:47:53,818 : INFO : downsampling leaves estimated 64999023 word corpus (68.5% of prior 94857217)\n",
      "2017-08-24 13:47:53,818 : INFO : estimated required memory for 54370 words and 100 dimensions: 67266733400 bytes\n",
      "2017-08-24 13:47:53,837 : INFO : Loading a fresh vocabulary\n",
      "2017-08-24 13:47:53,935 : INFO : min_count=7 retains 50193 unique words (38% of original 130707, drops 80514)\n",
      "2017-08-24 13:47:53,939 : INFO : min_count=7 leaves 94832155 word corpus (99% of original 95007318, drops 175163)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_count: 6, size of vocab:  38835.71428571428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-24 13:47:54,081 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2017-08-24 13:47:54,081 : INFO : downsampling leaves estimated 64970879 word corpus (68.5% of prior 94832155)\n",
      "2017-08-24 13:47:54,097 : INFO : estimated required memory for 50193 words and 100 dimensions: 67261303300 bytes\n",
      "2017-08-24 13:47:54,105 : INFO : Loading a fresh vocabulary\n",
      "2017-08-24 13:47:54,213 : INFO : min_count=8 retains 47567 unique words (36% of original 130707, drops 83140)\n",
      "2017-08-24 13:47:54,213 : INFO : min_count=8 leaves 94813773 word corpus (99% of original 95007318, drops 193545)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_count: 7, size of vocab:  35852.142857142855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-24 13:47:54,372 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2017-08-24 13:47:54,372 : INFO : downsampling leaves estimated 64950236 word corpus (68.5% of prior 94813773)\n",
      "2017-08-24 13:47:54,388 : INFO : estimated required memory for 47567 words and 100 dimensions: 67257889500 bytes\n",
      "2017-08-24 13:47:54,394 : INFO : Loading a fresh vocabulary\n",
      "2017-08-24 13:47:54,489 : INFO : min_count=9 retains 44343 unique words (33% of original 130707, drops 86364)\n",
      "2017-08-24 13:47:54,489 : INFO : min_count=9 leaves 94787981 word corpus (99% of original 95007318, drops 219337)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_count: 8, size of vocab:  33976.42857142857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-24 13:47:54,647 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2017-08-24 13:47:54,647 : INFO : downsampling leaves estimated 64921272 word corpus (68.5% of prior 94787981)\n",
      "2017-08-24 13:47:54,647 : INFO : estimated required memory for 44343 words and 100 dimensions: 67253698300 bytes\n",
      "2017-08-24 13:47:54,658 : INFO : Loading a fresh vocabulary\n",
      "2017-08-24 13:47:54,764 : INFO : min_count=10 retains 42369 unique words (32% of original 130707, drops 88338)\n",
      "2017-08-24 13:47:54,769 : INFO : min_count=10 leaves 94770215 word corpus (99% of original 95007318, drops 237103)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_count: 9, size of vocab:  31673.571428571428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-24 13:47:54,907 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2017-08-24 13:47:54,923 : INFO : downsampling leaves estimated 64901321 word corpus (68.5% of prior 94770215)\n",
      "2017-08-24 13:47:54,923 : INFO : estimated required memory for 42369 words and 100 dimensions: 67251132100 bytes\n",
      "2017-08-24 13:47:54,931 : INFO : Loading a fresh vocabulary\n",
      "2017-08-24 13:47:55,036 : INFO : min_count=11 retains 40516 unique words (30% of original 130707, drops 90191)\n",
      "2017-08-24 13:47:55,036 : INFO : min_count=11 leaves 94751685 word corpus (99% of original 95007318, drops 255633)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_count: 10, size of vocab:  30263.571428571428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-24 13:47:55,174 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2017-08-24 13:47:55,178 : INFO : downsampling leaves estimated 64880511 word corpus (68.5% of prior 94751685)\n",
      "2017-08-24 13:47:55,178 : INFO : estimated required memory for 40516 words and 100 dimensions: 67248723200 bytes\n",
      "2017-08-24 13:47:55,185 : INFO : Loading a fresh vocabulary\n",
      "2017-08-24 13:47:55,299 : INFO : min_count=12 retains 39099 unique words (29% of original 130707, drops 91608)\n",
      "2017-08-24 13:47:55,299 : INFO : min_count=12 leaves 94736098 word corpus (99% of original 95007318, drops 271220)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_count: 11, size of vocab:  28940.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-24 13:47:55,429 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2017-08-24 13:47:55,429 : INFO : downsampling leaves estimated 64863007 word corpus (68.5% of prior 94736098)\n",
      "2017-08-24 13:47:55,429 : INFO : estimated required memory for 39099 words and 100 dimensions: 67246881100 bytes\n",
      "2017-08-24 13:47:55,445 : INFO : Loading a fresh vocabulary\n",
      "2017-08-24 13:47:55,541 : INFO : min_count=13 retains 37353 unique words (28% of original 130707, drops 93354)\n",
      "2017-08-24 13:47:55,541 : INFO : min_count=13 leaves 94715146 word corpus (99% of original 95007318, drops 292172)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_count: 12, size of vocab:  27927.85714285714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-24 13:47:55,646 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2017-08-24 13:47:55,662 : INFO : downsampling leaves estimated 64839477 word corpus (68.5% of prior 94715146)\n",
      "2017-08-24 13:47:55,665 : INFO : estimated required memory for 37353 words and 100 dimensions: 67244611300 bytes\n",
      "2017-08-24 13:47:55,673 : INFO : Loading a fresh vocabulary\n",
      "2017-08-24 13:47:55,781 : INFO : min_count=14 retains 36170 unique words (27% of original 130707, drops 94537)\n",
      "2017-08-24 13:47:55,781 : INFO : min_count=14 leaves 94699767 word corpus (99% of original 95007318, drops 307551)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_count: 13, size of vocab:  26680.714285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-24 13:47:55,909 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2017-08-24 13:47:55,909 : INFO : downsampling leaves estimated 64822206 word corpus (68.5% of prior 94699767)\n",
      "2017-08-24 13:47:55,915 : INFO : estimated required memory for 36170 words and 100 dimensions: 67243073400 bytes\n",
      "2017-08-24 13:47:55,919 : INFO : Loading a fresh vocabulary\n",
      "2017-08-24 13:47:56,000 : INFO : min_count=15 retains 34972 unique words (26% of original 130707, drops 95735)\n",
      "2017-08-24 13:47:56,000 : INFO : min_count=15 leaves 94682995 word corpus (99% of original 95007318, drops 324323)\n",
      "2017-08-24 13:47:56,111 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2017-08-24 13:47:56,111 : INFO : downsampling leaves estimated 64803371 word corpus (68.4% of prior 94682995)\n",
      "2017-08-24 13:47:56,111 : INFO : estimated required memory for 34972 words and 100 dimensions: 67241516000 bytes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_count: 14, size of vocab:  25835.714285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-24 13:47:56,128 : INFO : Loading a fresh vocabulary\n",
      "2017-08-24 13:47:56,228 : INFO : min_count=16 retains 33905 unique words (25% of original 130707, drops 96802)\n",
      "2017-08-24 13:47:56,228 : INFO : min_count=16 leaves 94666990 word corpus (99% of original 95007318, drops 340328)\n",
      "2017-08-24 13:47:56,337 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2017-08-24 13:47:56,337 : INFO : downsampling leaves estimated 64785397 word corpus (68.4% of prior 94666990)\n",
      "2017-08-24 13:47:56,337 : INFO : estimated required memory for 33905 words and 100 dimensions: 67240128900 bytes\n",
      "2017-08-24 13:47:56,344 : INFO : Loading a fresh vocabulary\n",
      "2017-08-24 13:47:56,438 : INFO : min_count=17 retains 32786 unique words (25% of original 130707, drops 97921)\n",
      "2017-08-24 13:47:56,438 : INFO : min_count=17 leaves 94649086 word corpus (99% of original 95007318, drops 358232)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_count: 15, size of vocab:  24980.0\n",
      "min_count: 16, size of vocab:  24217.85714285714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-24 13:47:56,548 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2017-08-24 13:47:56,548 : INFO : downsampling leaves estimated 64765290 word corpus (68.4% of prior 94649086)\n",
      "2017-08-24 13:47:56,548 : INFO : estimated required memory for 32786 words and 100 dimensions: 67238674200 bytes\n",
      "2017-08-24 13:47:56,548 : INFO : Loading a fresh vocabulary\n",
      "2017-08-24 13:47:56,659 : INFO : min_count=18 retains 31976 unique words (24% of original 130707, drops 98731)\n",
      "2017-08-24 13:47:56,659 : INFO : min_count=18 leaves 94635316 word corpus (99% of original 95007318, drops 372002)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_count: 17, size of vocab:  23418.571428571428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-24 13:47:56,792 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2017-08-24 13:47:56,808 : INFO : downsampling leaves estimated 64749825 word corpus (68.4% of prior 94635316)\n",
      "2017-08-24 13:47:56,812 : INFO : estimated required memory for 31976 words and 100 dimensions: 67237621200 bytes\n",
      "2017-08-24 13:47:56,820 : INFO : Loading a fresh vocabulary\n",
      "2017-08-24 13:47:56,998 : INFO : min_count=19 retains 31038 unique words (23% of original 130707, drops 99669)\n",
      "2017-08-24 13:47:56,998 : INFO : min_count=19 leaves 94618432 word corpus (99% of original 95007318, drops 388886)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_count: 18, size of vocab:  22840.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-24 13:47:57,150 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2017-08-24 13:47:57,150 : INFO : downsampling leaves estimated 64730863 word corpus (68.4% of prior 94618432)\n",
      "2017-08-24 13:47:57,150 : INFO : estimated required memory for 31038 words and 100 dimensions: 67236401800 bytes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_count: 19, size of vocab:  22170.0\n"
     ]
    }
   ],
   "source": [
    "for num in range(0, 20):\n",
    "    print('min_count: {}, size of vocab: '.format(num), pre.scale_vocab(min_count=num, dry_run=True)['memory']['vocab']/700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Training the Doc2vec Model\n",
    "##PV-DBOW: paragraph vector - Distributed Bag of Words\n",
    "##PV-DM: paragraph vector - Distributed Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "# try both PV-DBOW and PV-DM model\n",
    "'''\n",
    "models = [\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, dbow_words=1, size=300, window=10, min_count=0, workers=cores),\n",
    "    # PV-DM w/average\n",
    "    Doc2Vec(dm=1, dm_mean=1, size=300, window=10, min_count=0, workers=cores),\n",
    "]\n",
    "'''\n",
    "model = Doc2Vec(alpha=0.025, min_alpha=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-24 16:00:19,410 : INFO : collecting all words and their counts\n",
      "2017-08-24 16:00:19,420 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2017-08-24 16:00:26,324 : INFO : PROGRESS: at example #10000, processed 16291689 words (2357606/s), 52638 word types, 163188390 tags\n",
      "2017-08-24 16:00:34,067 : INFO : PROGRESS: at example #20000, processed 32621419 words (2110771/s), 75605 word types, 163188390 tags\n",
      "2017-08-24 16:00:48,887 : INFO : PROGRESS: at example #30000, processed 49361312 words (1129206/s), 92632 word types, 163455185 tags\n",
      "2017-08-24 16:00:58,870 : INFO : PROGRESS: at example #40000, processed 66737559 words (1741993/s), 109277 word types, 163455185 tags\n",
      "2017-08-24 16:01:07,096 : INFO : PROGRESS: at example #50000, processed 82367850 words (1898730/s), 122903 word types, 167990131 tags\n",
      "2017-08-24 16:01:15,256 : INFO : collected 130707 word types and 167990131 unique tags from a corpus of 58860 examples and 95007318 words\n",
      "2017-08-24 16:01:15,272 : INFO : Loading a fresh vocabulary\n",
      "2017-08-24 16:03:14,133 : INFO : min_count=0 retains 130707 unique words (100% of original 130707, drops 0)\n",
      "2017-08-24 16:03:14,179 : INFO : min_count=0 leaves 95007318 word corpus (100% of original 95007318, drops 0)\n",
      "2017-08-24 16:03:15,405 : INFO : deleting the raw counts dictionary of 130707 items\n",
      "2017-08-24 16:03:15,425 : INFO : sample=0.001 downsamples 50 most-common words\n",
      "2017-08-24 16:03:15,428 : INFO : downsampling leaves estimated 65167521 word corpus (68.6% of prior 95007318)\n",
      "2017-08-24 16:03:15,433 : INFO : estimated required memory for 130707 words and 200 dimensions: 134666589500 bytes\n",
      "2017-08-24 16:03:16,732 : INFO : resetting layer weights\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-68e43e7e28b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatent_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[1;34m(self, sentences, keep_raw_vocab, trim_rule, progress_per, update)\u001b[0m\n\u001b[0;32m    551\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# initial survey\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# trim by min_count & precalculate downsampling\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinalize_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# build tables & arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mscan_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mfinalize_vocab\u001b[1;34m(self, update)\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[1;31m# set initial input/projection and hidden weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 726\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    727\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mreset_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    660\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"using concatenative %d-dimensional layer1\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer1_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 662\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    663\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    664\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mreset_weights\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    388\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoctag_syn0_lockf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoctag_syn0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoctag_syn0_lockf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# zeros suppress learning\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.build_vocab(patent_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-24 16:03:54,103 : INFO : training model with 4 workers on 130707 vocabulary and 200 features, using sg=1 hs=0 sample=0.001 negative=5 window=8\n",
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 837, in worker_loop\n",
      "    tally, raw_tally = self._do_train_job(sentences, alpha, (work, neu1))\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\doc2vec.py\", line 714, in _do_train_job\n",
      "    indexed_doctags = self.docvecs.indexed_doctags(doc.tags)\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\doc2vec.py\", line 313, in indexed_doctags\n",
      "    self.doctag_syn0, self.doctag_syn0_lockf, doctag_tokens)\n",
      "AttributeError: 'DocvecsArray' object has no attribute 'doctag_syn0'\n",
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 837, in worker_loop\n",
      "    tally, raw_tally = self._do_train_job(sentences, alpha, (work, neu1))\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\doc2vec.py\", line 714, in _do_train_job\n",
      "    indexed_doctags = self.docvecs.indexed_doctags(doc.tags)\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\doc2vec.py\", line 313, in indexed_doctags\n",
      "    self.doctag_syn0, self.doctag_syn0_lockf, doctag_tokens)\n",
      "AttributeError: 'DocvecsArray' object has no attribute 'doctag_syn0'\n",
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 837, in worker_loop\n",
      "    tally, raw_tally = self._do_train_job(sentences, alpha, (work, neu1))\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\doc2vec.py\", line 714, in _do_train_job\n",
      "    indexed_doctags = self.docvecs.indexed_doctags(doc.tags)\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\doc2vec.py\", line 313, in indexed_doctags\n",
      "    self.doctag_syn0, self.doctag_syn0_lockf, doctag_tokens)\n",
      "AttributeError: 'DocvecsArray' object has no attribute 'doctag_syn0'\n",
      "Exception in thread Thread-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 837, in worker_loop\n",
      "    tally, raw_tally = self._do_train_job(sentences, alpha, (work, neu1))\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\doc2vec.py\", line 714, in _do_train_job\n",
      "    indexed_doctags = self.docvecs.indexed_doctags(doc.tags)\n",
      "  File \"c:\\users\\jasoliu\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\doc2vec.py\", line 313, in indexed_doctags\n",
      "    self.doctag_syn0, self.doctag_syn0_lockf, doctag_tokens)\n",
      "AttributeError: 'DocvecsArray' object has no attribute 'doctag_syn0'\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train(patent_docs)\n",
    "    model.alpha -= 0.002\n",
    "    model.min_alpha = model.alpha\n",
    "    #%%time model.train(patent_docs, total_examples=model.corpus_count, epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('D2V')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
